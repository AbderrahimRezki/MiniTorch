# MiniTorch

**MiniTorch** is a minimalistic autograd engine inspired by [micrograd](https://github.com/karpathy/micrograd), offering a PyTorch-like API for building and training neural networks.
It's designed as an educational tool to help understand the core mechanics of automatic differentiation and neural network training.

## Features

- **Autograd Engine**: Implements reverse-mode automatic differentiation to compute gradients.
- **Neural Network Modules**: Provides basic building blocks like layers and activation functions.
- **Training Loop**: Includes a simple training loop to train models on datasets.
- **PyTorch-like API**: Offers an interface similar to PyTorch for ease of use and familiarity.

## Installation

1. **Clone the repository**:

   ```bash
   git clone https://github.com/AbderrahimRezki/MiniTorch.git
   cd MiniTorch
   ```

2. **Create a virtual environment** (optional but recommended):

   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install the package**:

   ```bash
   pip install -e .
   ```

## Contributing

Contributions are welcome! If you have suggestions or improvements, feel free to open an issue or submit a pull request.
